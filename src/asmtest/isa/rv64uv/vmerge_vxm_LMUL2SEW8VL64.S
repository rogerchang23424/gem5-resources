
# See LICENSE for license details.

# This file is automatically generated. Do not edit.

#*****************************************************************************
# isa/rv64uv/vmerge_vxm_LMUL2SEW8VL64.S
#-----------------------------------------------------------------------------
#
# Test vmerge.vxm instructions.
# With LMUL=2, SEW=8, VL=64
#

#include "riscv_test.h"
#include "test_macros.h"

RVTEST_RV64UV

RVTEST_CODE_BEGIN

  la a1, tdat
  la a3, sres
  lbu a4, 0(a1)
  li a5, 1

  sb a5, 0(a3)
  lbu a4, 1(a1)
  li a5, 1
  lbu a5, 1(a1)
  sb a5, 1(a3)
  lbu a4, 2(a1)
  li a5, 1

  sb a5, 2(a3)
  lbu a4, 3(a1)
  li a5, 1
  lbu a5, 3(a1)
  sb a5, 3(a3)
  lbu a4, 4(a1)
  li a5, 1

  sb a5, 4(a3)
  lbu a4, 5(a1)
  li a5, 1
  lbu a5, 5(a1)
  sb a5, 5(a3)
  lbu a4, 6(a1)
  li a5, 1

  sb a5, 6(a3)
  lbu a4, 7(a1)
  li a5, 1
  lbu a5, 7(a1)
  sb a5, 7(a3)
  lbu a4, 8(a1)
  li a5, 1

  sb a5, 8(a3)
  lbu a4, 9(a1)
  li a5, 1
  lbu a5, 9(a1)
  sb a5, 9(a3)
  lbu a4, 10(a1)
  li a5, 1

  sb a5, 10(a3)
  lbu a4, 11(a1)
  li a5, 1
  lbu a5, 11(a1)
  sb a5, 11(a3)
  lbu a4, 12(a1)
  li a5, 1

  sb a5, 12(a3)
  lbu a4, 13(a1)
  li a5, 1
  lbu a5, 13(a1)
  sb a5, 13(a3)
  lbu a4, 14(a1)
  li a5, 1

  sb a5, 14(a3)
  lbu a4, 15(a1)
  li a5, 1
  lbu a5, 15(a1)
  sb a5, 15(a3)
  lbu a4, 16(a1)
  li a5, 1

  sb a5, 16(a3)
  lbu a4, 17(a1)
  li a5, 1
  lbu a5, 17(a1)
  sb a5, 17(a3)
  lbu a4, 18(a1)
  li a5, 1

  sb a5, 18(a3)
  lbu a4, 19(a1)
  li a5, 1
  lbu a5, 19(a1)
  sb a5, 19(a3)
  lbu a4, 20(a1)
  li a5, 1

  sb a5, 20(a3)
  lbu a4, 21(a1)
  li a5, 1
  lbu a5, 21(a1)
  sb a5, 21(a3)
  lbu a4, 22(a1)
  li a5, 1

  sb a5, 22(a3)
  lbu a4, 23(a1)
  li a5, 1
  lbu a5, 23(a1)
  sb a5, 23(a3)
  lbu a4, 24(a1)
  li a5, 1

  sb a5, 24(a3)
  lbu a4, 25(a1)
  li a5, 1
  lbu a5, 25(a1)
  sb a5, 25(a3)
  lbu a4, 26(a1)
  li a5, 1

  sb a5, 26(a3)
  lbu a4, 27(a1)
  li a5, 1
  lbu a5, 27(a1)
  sb a5, 27(a3)
  lbu a4, 28(a1)
  li a5, 1

  sb a5, 28(a3)
  lbu a4, 29(a1)
  li a5, 1
  lbu a5, 29(a1)
  sb a5, 29(a3)
  lbu a4, 30(a1)
  li a5, 1

  sb a5, 30(a3)
  lbu a4, 31(a1)
  li a5, 1
  lbu a5, 31(a1)
  sb a5, 31(a3)
  lbu a4, 32(a1)
  li a5, 1

  sb a5, 32(a3)
  lbu a4, 33(a1)
  li a5, 1
  lbu a5, 33(a1)
  sb a5, 33(a3)
  lbu a4, 34(a1)
  li a5, 1

  sb a5, 34(a3)
  lbu a4, 35(a1)
  li a5, 1
  lbu a5, 35(a1)
  sb a5, 35(a3)
  lbu a4, 36(a1)
  li a5, 1

  sb a5, 36(a3)
  lbu a4, 37(a1)
  li a5, 1
  lbu a5, 37(a1)
  sb a5, 37(a3)
  lbu a4, 38(a1)
  li a5, 1

  sb a5, 38(a3)
  lbu a4, 39(a1)
  li a5, 1
  lbu a5, 39(a1)
  sb a5, 39(a3)
  lbu a4, 40(a1)
  li a5, 1

  sb a5, 40(a3)
  lbu a4, 41(a1)
  li a5, 1
  lbu a5, 41(a1)
  sb a5, 41(a3)
  lbu a4, 42(a1)
  li a5, 1

  sb a5, 42(a3)
  lbu a4, 43(a1)
  li a5, 1
  lbu a5, 43(a1)
  sb a5, 43(a3)
  lbu a4, 44(a1)
  li a5, 1

  sb a5, 44(a3)
  lbu a4, 45(a1)
  li a5, 1
  lbu a5, 45(a1)
  sb a5, 45(a3)
  lbu a4, 46(a1)
  li a5, 1

  sb a5, 46(a3)
  lbu a4, 47(a1)
  li a5, 1
  lbu a5, 47(a1)
  sb a5, 47(a3)
  lbu a4, 48(a1)
  li a5, 1

  sb a5, 48(a3)
  lbu a4, 49(a1)
  li a5, 1
  lbu a5, 49(a1)
  sb a5, 49(a3)
  lbu a4, 50(a1)
  li a5, 1

  sb a5, 50(a3)
  lbu a4, 51(a1)
  li a5, 1
  lbu a5, 51(a1)
  sb a5, 51(a3)
  lbu a4, 52(a1)
  li a5, 1

  sb a5, 52(a3)
  lbu a4, 53(a1)
  li a5, 1
  lbu a5, 53(a1)
  sb a5, 53(a3)
  lbu a4, 54(a1)
  li a5, 1

  sb a5, 54(a3)
  lbu a4, 55(a1)
  li a5, 1
  lbu a5, 55(a1)
  sb a5, 55(a3)
  lbu a4, 56(a1)
  li a5, 1

  sb a5, 56(a3)
  lbu a4, 57(a1)
  li a5, 1
  lbu a5, 57(a1)
  sb a5, 57(a3)
  lbu a4, 58(a1)
  li a5, 1

  sb a5, 58(a3)
  lbu a4, 59(a1)
  li a5, 1
  lbu a5, 59(a1)
  sb a5, 59(a3)
  lbu a4, 60(a1)
  li a5, 1

  sb a5, 60(a3)
  lbu a4, 61(a1)
  li a5, 1
  lbu a5, 61(a1)
  sb a5, 61(a3)
  lbu a4, 62(a1)
  li a5, 1

  sb a5, 62(a3)
  lbu a4, 63(a1)
  li a5, 1
  lbu a5, 63(a1)
  sb a5, 63(a3)


  li t0, -1
  vsetvli t1, t0, e8,m2,ta,ma
  la a2, tdat
  vle8.v v4, (a2)
  vle8.v v2, (a2)

  
  li t0, -1
  vsetvli t1, t0, e8,m1,ta,ma
  la a3, mask
  vle8.v v0, (a3)

  li t0, 64
  vsetvli t1, t0, e8,m2,ta,ma
  li t2, 1
  vmerge.vxm v2, v4, t2, v0

  li t0, -1
  vsetvli t1, t0, e8,m2,ta,ma
  la a1, res
  vse8.v v2, (a1)
  la a2, sres

  TEST_CASE_REG(3, t0, t1, ld t0, 0(a1); ld t1, 0(a2); addi a1, a1, 8; addi a2, a2, 8)
  TEST_CASE_REG(4, t0, t1, ld t0, 0(a1); ld t1, 0(a2); addi a1, a1, 8; addi a2, a2, 8)
  TEST_CASE_REG(5, t0, t1, ld t0, 0(a1); ld t1, 0(a2); addi a1, a1, 8; addi a2, a2, 8)
  TEST_CASE_REG(6, t0, t1, ld t0, 0(a1); ld t1, 0(a2); addi a1, a1, 8; addi a2, a2, 8)
  TEST_CASE_REG(7, t0, t1, ld t0, 0(a1); ld t1, 0(a2); addi a1, a1, 8; addi a2, a2, 8)
  TEST_CASE_REG(8, t0, t1, ld t0, 0(a1); ld t1, 0(a2); addi a1, a1, 8; addi a2, a2, 8)
  TEST_CASE_REG(9, t0, t1, ld t0, 0(a1); ld t1, 0(a2); addi a1, a1, 8; addi a2, a2, 8)
  TEST_CASE_REG(10, t0, t1, ld t0, 0(a1); ld t1, 0(a2); addi a1, a1, 8; addi a2, a2, 8)



  li t0, -1
  vsetvli t1, t0, e8,m2,ta,ma
  la a2, tdat
  vle8.v v4, (a2)
  vle8.v v2, (a2)

  
  li t0, -1
  vsetvli t1, t0, e8,m1,ta,ma
  la a3, mask
  vle8.v v0, (a3)

  li t0, 64
  vsetvli t1, t0, e8,m2,tu,ma
  li t2, 1
  vmerge.vxm v2, v4, t2, v0

  li t0, -1
  vsetvli t1, t0, e8,m2,ta,ma
  la a1, res
  vse8.v v2, (a1)
  la a2, sres

  TEST_CASE_REG(11, t0, t1, ld t0, 0(a1); ld t1, 0(a2); addi a1, a1, 8; addi a2, a2, 8)
  TEST_CASE_REG(12, t0, t1, ld t0, 0(a1); ld t1, 0(a2); addi a1, a1, 8; addi a2, a2, 8)
  TEST_CASE_REG(13, t0, t1, ld t0, 0(a1); ld t1, 0(a2); addi a1, a1, 8; addi a2, a2, 8)
  TEST_CASE_REG(14, t0, t1, ld t0, 0(a1); ld t1, 0(a2); addi a1, a1, 8; addi a2, a2, 8)
  TEST_CASE_REG(15, t0, t1, ld t0, 0(a1); ld t1, 0(a2); addi a1, a1, 8; addi a2, a2, 8)
  TEST_CASE_REG(16, t0, t1, ld t0, 0(a1); ld t1, 0(a2); addi a1, a1, 8; addi a2, a2, 8)
  TEST_CASE_REG(17, t0, t1, ld t0, 0(a1); ld t1, 0(a2); addi a1, a1, 8; addi a2, a2, 8)
  TEST_CASE_REG(18, t0, t1, ld t0, 0(a1); ld t1, 0(a2); addi a1, a1, 8; addi a2, a2, 8)



  TEST_PASSFAIL

RVTEST_CODE_END

  .data
RVTEST_DATA_BEGIN

res:
  .zero 72

sres:
  .zero 72

tdat:
  .quad 0x103f8ffefefff
  .quad 0x1070001000100
  .quad 0x103f8ffefefff
  .quad 0x1070001000100
  .quad 0x103f8ffefefff
  .quad 0x1070001000100
  .quad 0x103f8ffefefff
  .quad 0x1070001000100
  .quad 0x103f8ffefefff

mask:
  .quad 0x5555555555555555
  .quad 0x5555555555555555
  .quad 0x5555555555555555
  .quad 0x5555555555555555

RVTEST_DATA_END
